{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "025a9d6d-cdfa-4c94-bbe1-2ed1a21a72b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        self.sa = nn.MultiheadAttention(embed_dim=n_embed, num_heads=n_head, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.sa(self.ln1(x), self.ln1(x), self.ln1(x), need_weights=False)\n",
    "        x = x + attn_output\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTMini(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_embed=128, n_head=4, n_layer=2):\n",
    "        super().__init__()\n",
    "        assert n_embed % n_head == 0\n",
    "        self.token_embed = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_embed = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        token_emb = self.token_embed(idx)\n",
    "        pos = torch.arange(T, device=idx.device)\n",
    "        pos_emb = self.pos_embed(pos).unsqueeze(0)\n",
    "        x = token_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1b235a2-743f-4334-ab9f-abff7e3734ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®le charg√©.\n"
     ]
    }
   ],
   "source": [
    "# Chargement\n",
    "checkpoint = torch.load('gpt_mini_checkpoint.pt')\n",
    "\n",
    "stoi = checkpoint['stoi']\n",
    "itos = checkpoint['itos']\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# Recr√©er le mod√®le\n",
    "model = GPTMini(\n",
    "    vocab_size=checkpoint['vocab_size'],\n",
    "    block_size=checkpoint['block_size'],\n",
    "    n_embed=128,\n",
    "    n_head=4\n",
    ")\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úÖ Mod√®le charg√©.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0f76fa5-3ce0-412d-9ec2-7b71649e38c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÆ Texte g√©n√©r√© :\n",
      "\n",
      "un matin,  n n    n    n    n n   n             n                                                                                                                                     c                          \n"
     ]
    }
   ],
   "source": [
    "# G√©n√©rateur de texte\n",
    "@torch.no_grad()\n",
    "def generate(model, prompt, max_new_tokens=100, temperature=1.0):\n",
    "    idx = torch.tensor([encode(prompt)], dtype=torch.long)\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -checkpoint['block_size']:]\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "# Exemple\n",
    "prompt = \"un matin,\"\n",
    "print(\"\\nüîÆ Texte g√©n√©r√© :\\n\")\n",
    "print(generate(model, prompt, max_new_tokens=200, temperature=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6238b2f9-0dc2-400e-8a80-96ff62d6155a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
