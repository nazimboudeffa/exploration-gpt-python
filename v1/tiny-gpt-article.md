âœ¨ GÃ©nÃ©rer du texte avec un mini GPT en PyTorch

ğŸ“š Introduction

Les modÃ¨les de langage de type GPT (Generative Pretrained Transformers) sont capables de gÃ©nÃ©rer du texte de maniÃ¨re fluide Ã  partir dâ€™un simple prompt. Ici, nous allons construire une version trÃ¨s simplifiÃ©e dâ€™un tel modÃ¨le en PyTorch, entraÃ®nÃ©e caractÃ¨re par caractÃ¨re Ã  partir dâ€™un fichier texte.

Ce projet couvre les Ã©tapes suivantes :

1-Lecture et encodage dâ€™un texte
2-PrÃ©paration des batchs avec DataLoader
3-DÃ©finition dâ€™un mini-modÃ¨le GPT
4-EntraÃ®nement

5-GÃ©nÃ©ration de texte avec sampling contrÃ´lÃ© (temperature, top-k, top-p)

ğŸ“ PrÃ©traitement du texte
On commence par charger un fichier texte (exemple.txt), nettoyer le contenu, et encoder chaque caractÃ¨re en entier. On crÃ©e un vocabulaire simple basÃ© sur les caractÃ¨res uniques.

```
with open('exemple.txt', 'r', encoding='utf-8') as f:
    text = f.read().strip().lower()

chars = sorted(set(text))
stoi = {ch: i for i, ch in enumerate(chars)}
itos = {i: ch for ch, i in stoi.items()}
data = torch.tensor([stoi[c] for c in text], dtype=torch.long)
```

ğŸ“¦ GÃ©nÃ©ration de batchs avec Dataset et DataLoader

PlutÃ´t que de coder manuellement la gÃ©nÃ©ration des batchs, on utilise les outils standard de PyTorch :

```

class CharDataset(torch.utils.data.Dataset):
    def __init__(self, data, block_size):
        self.data = data
        self.block_size = block_size

    def __len__(self):
        return len(data) - block_size

    def __getitem__(self, idx):
        x = self.data[idx:idx+block_size]
        y = self.data[idx+1:idx+1+block_size]
        return x, y

dataset = CharDataset(data, block_size=64)
dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)

```

ğŸ§  Mini GPT : un modÃ¨le simple

Notre modÃ¨le se compose uniquement d'une couche d'embedding suivie dâ€™une couche linÃ©aire qui prÃ©dit le caractÃ¨re suivant :

```
class TinyGPT(nn.Module):
    def __init__(self, vocab_size, n_embed=64):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, n_embed)
        self.lm_head = nn.Linear(n_embed, vocab_size)

    def forward(self, idx):
        x = self.embed(idx)         # [batch, time, n_embed]
        logits = self.lm_head(x)    # [batch, time, vocab_size]
        return logits
```

âš™ï¸ EntraÃ®nement

Le modÃ¨le est entraÃ®nÃ© Ã  prÃ©dire chaque caractÃ¨re suivant dans les sÃ©quences fournies par le DataLoader.

```
model = TinyGPT(vocab_size)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)
loss_fn = nn.CrossEntropyLoss()

data_iter = iter(dataloader)
for step in range(500):
    try:
        x, y = next(data_iter)
    except StopIteration:
        data_iter = iter(dataloader)
        x, y = next(data_iter)

    logits = model(x)
    loss = loss_fn(logits.view(-1, vocab_size), y.view(-1))
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if step % 100 == 0:
        print(f"Ã‰tape {step} â€“ Perte : {loss.item():.4f}")
```

ğŸª„ GÃ©nÃ©ration de texte avec sampling contrÃ´lÃ©

La gÃ©nÃ©ration de texte se fait un caractÃ¨re Ã  la fois, Ã  partir dâ€™un prompt. On peut contrÃ´ler la crÃ©ativitÃ© avec :

- temperature : ajuste la "nettetÃ©" des probabilitÃ©s (baisse = plus conservateur)
- top_k : ne considÃ¨re que les k tokens les plus probables
- top_p : garde les tokens les plus probables jusquâ€™Ã  atteindre p de probabilitÃ© cumulÃ©e

```
@torch.no_grad()
def generate(model, prompt, max_new_tokens=100, temperature=1.0, top_k=None, top_p=None):
    model.eval()
    idx = torch.tensor([stoi[c] for c in prompt], dtype=torch.long).unsqueeze(0)

    for _ in range(max_new_tokens):
        idx_cond = idx[:, -block_size:]
        logits = model(idx_cond)[:, -1, :] / temperature

        if top_k is not None:
            vals, inds = torch.topk(logits, top_k)
            mask = torch.full_like(logits, float('-inf'))
            mask.scatter_(1, inds, vals)
            logits = mask

        if top_p is not None:
            sorted_logits, sorted_indices = torch.sort(logits, descending=True)
            probs = F.softmax(sorted_logits, dim=-1)
            cumprobs = torch.cumsum(probs, dim=-1)
            mask = cumprobs > top_p
            mask[..., 1:] = mask[..., :-1].clone()
            mask[..., 0] = 0
            logits[0, sorted_indices[0][mask[0]]] = float('-inf')

        probs = F.softmax(logits, dim=-1)
        next_id = torch.multinomial(probs, num_samples=1)
        idx = torch.cat([idx, next_id], dim=1)

    return ''.join([itos[i.item()] for i in idx[0]])
```

ğŸ§ª Exemple dâ€™utilisation

```
prompt = "un matin,"
text = generate(model, prompt, max_new_tokens=200, temperature=0.8, top_k=30, top_p=0.9)
print(text)
```

âœ… RÃ©sultat

Avec un entraÃ®nement de quelques centaines dâ€™itÃ©rations, le modÃ¨le peut dÃ©jÃ  gÃ©nÃ©rer un texte "cohÃ©rent" dans le style du corpus utilisÃ©.

ğŸ”š Conclusion

Ce mini-GPT montre quâ€™il est possible de gÃ©nÃ©rer du texte avec un modÃ¨le trÃ¨s simple, sans transformer complet ni multi-tÃªtes. Il est parfait pour l'apprentissage des fondamentaux du machine learning sur du texte. On peut ensuite enrichir :

-En ajoutant des blocs de transformer
-En entraÃ®nant sur des mots ou des tokens (niveau BPE)
-En sauvegardant les checkpoints
-En dÃ©ployant une interface Web (Streamlit, Gradio...)