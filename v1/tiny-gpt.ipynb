{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae0ca814-f2c5-491c-961b-268ccb89e5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte encodÃ©. Taille vocabulaire : 36\n",
      "x shape : torch.Size([4, 64])\n",
      "y shape : torch.Size([4, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Charger le fichier texte\n",
    "with open('exemple.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "text = text.strip().lower()\n",
    "\n",
    "# 2. Vocabulaire\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "# 3. Encodage\n",
    "data = torch.tensor([stoi[c] for c in text], dtype=torch.long)\n",
    "block_size = 64\n",
    "\n",
    "# 4. Dataset personnalisÃ©\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx : idx + self.block_size]\n",
    "        y = self.data[idx + 1 : idx + 1 + self.block_size]\n",
    "        return x, y\n",
    "\n",
    "# 5. CrÃ©er le DataLoader\n",
    "batch_size = 4\n",
    "dataset = CharDataset(data, block_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 6. Exemple dâ€™utilisation\n",
    "x, y = next(iter(dataloader))\n",
    "print(\"Texte encodÃ©. Taille vocabulaire :\", vocab_size)\n",
    "print(\"x shape :\", x.shape)  # [batch_size, block_size]\n",
    "print(\"y shape :\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd02c635-c812-4895-87d7-76114e157f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ã‰tape 0 â€“ Perte : 3.7334\n",
      "Ã‰tape 100 â€“ Perte : 2.5172\n",
      "Ã‰tape 200 â€“ Perte : 2.2097\n",
      "Ã‰tape 300 â€“ Perte : 2.1297\n",
      "Ã‰tape 400 â€“ Perte : 2.0330\n",
      "âœ… EntraÃ®nement terminÃ©.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# DÃ©finition dâ€™un mini-modÃ¨le GPT\n",
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed=64):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        x = self.embed(idx)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "# Initialisation\n",
    "model = TinyGPT(vocab_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# PrÃ©parer un itÃ©rateur persistant sur le dataloader\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "# EntraÃ®nement simple\n",
    "for step in range(500):\n",
    "    try:\n",
    "        x, y = next(data_iter)\n",
    "    except StopIteration:\n",
    "        data_iter = iter(dataloader)\n",
    "        x, y = next(data_iter)\n",
    "\n",
    "    logits = model(x)\n",
    "    B, T, C = logits.shape\n",
    "    loss = loss_fn(logits.view(B * T, C), y.view(B * T))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Ã‰tape {step} â€“ Perte : {loss.item():.4f}\")\n",
    "\n",
    "print(\"âœ… EntraÃ®nement terminÃ©.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cbd97e2-2c81-4b82-9706-cd85218126bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, prompt, max_new_tokens=100, temperature=1.0, top_k=None, top_p=None):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([stoi[c] for c in prompt], dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / temperature  # tempÃ©rature\n",
    "\n",
    "        # Appliquer top-k\n",
    "        if top_k is not None:\n",
    "            topk_vals, topk_idx = torch.topk(logits, top_k)\n",
    "            logits_filtered = torch.full_like(logits, float('-inf'))\n",
    "            logits_filtered.scatter_(1, topk_idx, topk_vals)\n",
    "            logits = logits_filtered\n",
    "\n",
    "        # Appliquer top-p (nucleus)\n",
    "        if top_p is not None:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "            # Masque les tokens dÃ©passant top_p\n",
    "            sorted_mask = cumulative_probs > top_p\n",
    "            sorted_mask[..., 1:] = sorted_mask[..., :-1].clone()\n",
    "            sorted_mask[..., 0] = 0  # Garder au moins 1\n",
    "\n",
    "            logits[0, sorted_indices[0][sorted_mask[0]]] = float('-inf')\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    out = ''.join([itos[i.item()] for i in idx[0]])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "807a302a-320a-4643-bce6-b5bc7ca2c2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”® Texte gÃ©nÃ©rÃ© :\n",
      "\n",
      "un matin, le le mourit le me le le moure le le me le le le le mme le le le le le le le le le le me le moure le le le le le le le le le le le me le le mmit le le le le le le le le le le le mount le mourit le le\n"
     ]
    }
   ],
   "source": [
    "prompt = \"un matin,\"\n",
    "generated_text = generate(model, prompt, max_new_tokens=200, temperature=0.1, top_k=20, top_p=0.9)\n",
    "\n",
    "print(\"ðŸ”® Texte gÃ©nÃ©rÃ© :\\n\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27673746-96f8-401e-9d97-523abbc1bf6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
